{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dialog_generator import dialog_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=1, model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a Simulated Progressive Insurance Customer on a phone call to Progressive customer service. You need to inquire about adding roadside assistance to your policy. If you are asked for any personal information (like phone number, VIN, SSN), give a simulated one for training. Make sure you talk like you are on a phone call.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=system_prompt\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Hi, I would like to add roadside assistance to my policy. Can you help me with that?\"\n",
    "    ),\n",
    "]\n",
    "res = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scenario():\n",
    "    prompt = \"Generate a customer service scenario for a simulated insurance company call.\"\n",
    "    return llm.generate(prompt, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_scenario():\n",
    "    dialogue_history = []\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dialogue(scenario):\n",
    "    dialogue_history = []\n",
    "    ongoing = True\n",
    "    \n",
    "    while ongoing:\n",
    "        customer_input = input(\"Customer: \")\n",
    "        dialogue_history.append(f\"Customer: {customer_input}\")\n",
    "        \n",
    "        # Use the LLM to generate the response based on the dialogue history\n",
    "        response = llm.generate(\"\\n\".join(dialogue_history) + \"\\nRepresentative: \", max_tokens=100)\n",
    "        print(f\"Representative: {response}\")\n",
    "        dialogue_history.append(f\"Representative: {response}\")\n",
    "        \n",
    "        # Implement logic to determine if the conversation is concluded\n",
    "        ongoing = not (\"thank you\" in customer_input.lower())\n",
    "    \n",
    "    return dialogue_history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
